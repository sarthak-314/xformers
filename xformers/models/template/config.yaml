scan_layers: True
remat: 'minimal'

# Model Architecture
vocab_size: 128100 # Same as DeBERTa
num_hidden_layers: 48
num_attention_heads: 16
layer_norm_epsilon: 1e-8

max_relative_positions: 512
position_buckets: 256

hidden_size: 1024
hidden_dropout_prob: 0.10
hidden_act: 'gelu'

intermediate_size: 4096
