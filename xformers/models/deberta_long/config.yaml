num_partitions: 8
remat_policy: 'minimal' # 'full', None
scan_layers: False

# Relative Position Embeddings
num_relative_position_embeddings: 512
relative_position_encoding_buckets: 256

# Self-Attention
self_attention_type: 'sliding_window' # 'full', 'sliding_window'
sliding_window_block_size: 512
attention_probs_dropout_rate: 0.10
num_attention_heads: 16

# Feed-Forward MLP
hidden_dropout_rate: 0.10
hidden_activation: 'gelu'
intermediate_size: 4096

num_hidden_layers: 24
hidden_size: 1024

share_attention_key: True
vocab_size: 128100
layer_norm_epsilon: 1e-7
initializer_range: 0.02